{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积神经网络和TensorFlow介绍\n",
    "\n",
    "本教程对CNN和TensorFlow做了非常简洁的介绍，与当前常见的TensorFlow 和CNN教材不同，我们更多的侧重于TensorFlow的后台机制。本教程基于计算机来阐述，其可用于TensorFlow的定义，执行，储存等。同样，它也适用于其他任务。\n",
    "\n",
    "教程安排如下:\n",
    "\n",
    "1. [基础TensorFlow应用](#基础TensorFlow应用)\n",
    "2. [TensorFlow中的计算图](#TensorFlow中的计算图)\n",
    "3. [作为计算图的CNN](#作为计算图的CNN)\n",
    "4. [练习](#练习)\n",
    "\n",
    "我们的教程针对的是TensorFlow的**graph execution**模式，与最近的TensorFlow版本引入的**eager execution**相比，它可以在不首先构建计算图的情况下即时执行计算。\n",
    "\n",
    "本教程仅介绍 inference任务。\n",
    "\n",
    "\n",
    "在以下教程中，我们将提供已构建的TensorFlow模型，请保存以便接下来的学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 效果预期\n",
    "\n",
    "1. 学会使用基础的TensorFlow。\n",
    "2. 理解TensorFlow模型结构并学会从模型中提取信息。\n",
    "3. 学会如何可视化CNN中间输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TensorFlow基础使用\n",
    "\n",
    "一方面，TensorFow是著名的机器学习构架，开发者能够轻松的在上面建立自己的模型。另一方面，TensorFow是嵌入在Python中的域特定语言（DSL）。作为域特定语言，TensorFlow提供了 primitives, 或者 APIs来构建计算机图用于机器学习模型。因此，在本教程中，我们采用TensorFlow作为编程语言：我们首先介绍TensorFlow中典型编程语言的元素，如语法，变量，数据类型，文字和运算符; 接下来，将展示如何执行TensorFlow构建的程序; 最后，会有一个关于使用TensorFlow的提示和技巧的简短列表\n",
    "\n",
    "我们将在以下部分中研究TensorFlow中的计算图机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本语法\n",
    "\n",
    "当你安装好TensorFlow，我们还需要在python中导入如下代码（若没有安装好，请按照本教程[tutorial](https://www.tensorflow.org/install/)进行安装）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow and set its alias as \"tf\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在TensorFlow DSL中的可操作基单元是 **tensor**（相关文档 [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor)）， 简单来说，tensor是一个N维数组，和NumPy中的[ndarray](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ndarray.html)类似。下面的例子展示了如何用常数来构建tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1.0, 2.0], [3.0, 4.0]]) # This is a 2D Matrix or a 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除常数外，还可以创建**变量**tensor, 点击[document](https://www.tensorflow.org/programmers_guide/variables)查看详细。可以通过`get_variable`创建变量的名称和大小，如下所示。Tensor名称在其[name scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)内只能是唯一的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'a:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_variable('a', [2, 2]) # This is tensor 'a' in a global TF scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们可以利用TensorFlow里的**算子**[operators](https://www.tensorflow.org/api_docs/python/tf/Operation)来构建计算。 `tf.matmul` 是一个直观的例子， 它在两个2维tensors之间执行矩阵乘法。TensorFlow算子返回的对象是表征计算结果的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(tf.constant([[1.0, 2.0], [3.0, 4.0]]), tf.constant([[1.0, 2.0], [3.0, 4.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要运行计算，即获得tensor中的内容，我们需要使用tf.Session（参见 [here](https://www.tensorflow.org/api_docs/python/tf/Session)）来初始化环境，以便来执行TensorFlow的程序。这部分不做深入探究。\n",
    "\n",
    "注意每个变量在开始前都需要初始化，下面的示例使用`random_normal_initializer`来初始化tensor b的内容，使其为随机值且服从正态分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2486579e+00  1.5110373e-03]\n",
      " [ 4.2668915e+00 -5.9322524e-01]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.get_variable('b', [2, 2], initializer=tf.random_normal_initializer())\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow中的计算图\n",
    "\n",
    "大多数的计算机程序都可以用一张计算图来表示。由TensorFlow构建的程序明确地初始化了它的计算图。由TensorFlow构建的计算图是[数据流图](https://www.tensorflow.org/programmers_guide/graphs#why_dataflow_graphs)，其中每个节点表示计算（或操作），每个边是数据。下面的gif显示了如何为双层MLP（多层感知器）构建数据流图。在第一层，输入数据将首先通过矩阵乘法(`tf.matmul`)，偏向量加法(`BiasAdd`)和非线性激励(`tf.nn.relu`)。第二层在相同的设置下，处理除ReLU激励函数外的输出数据。权重和偏差向量是可变tensor。 注意，此图中还初始化了训练节点（梯度，更新）。\n",
    "\n",
    "![](https://www.tensorflow.org/images/tensors_flowing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获得图像内容\n",
    "\n",
    "本教程的目的主要是为了了解TensorFlow的机制以进一步进行优化和部署。这意味着有必要访问我们构建的图中的内容。假设我们构建的图形与动画图形所示的计算完全相同，从输入到`Softmax`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph() # build a new graph\n",
    "with g.as_default():\n",
    "    # the second argument is the shape in this case 28x28x1 image\n",
    "    input_tensor = tf.placeholder(tf.float32, [28, 28, 1])\n",
    "    \n",
    "    # reshape it as a column vector\n",
    "    x = tf.reshape(input_tensor, [1, 784], name='x')\n",
    "    \n",
    "    # The first perceptron layer\n",
    "    W1 = tf.get_variable('W1', [784, 1024])\n",
    "    b1 = tf.get_variable('b1', [1024])\n",
    "    \n",
    "    # Perform the matrix multiply and add bias\n",
    "    y1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, W1), b1), name='y1')\n",
    "    \n",
    "    # The second perceptron layer\n",
    "    W2 = tf.get_variable('W2', [1024, 10])\n",
    "    b2 = tf.get_variable('b2', [10])\n",
    "    y2 = tf.nn.bias_add(tf.matmul(y1, W2), b2, name='y2')\n",
    "    \n",
    "    logits = tf.nn.softmax(y2, name='logits') # softmax output is named as \"logits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow把所有与图相关的信息都放在tf.Graph [class](https://www.tensorflow.org/api_docs/python/tf/Graph)中，就像上例中的`g`一样。该图形对象将图形的定义保存为[`GraphDef`](https://www.tensorflow.org/api_docs/python/tf/GraphDef)，可以通过`as_graph_def()`访问。你可以通过其`node`属性遍历`GraphDef`中的每个节点。 每个节点都有名称，`op`用于操作名称，`input`用于输入到当前节点的节点名称列表，以及其他可访问属性。在下面的例子中，我们输出每个`MatMul`节点的属性。定义节点的语法是一个[Protocol Buffer](https://developers.google.com/protocol-buffers/)，可以查阅其文档以获得更多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"MatMul\"\n",
      "op: \"MatMul\"\n",
      "input: \"x\"\n",
      "input: \"W1/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_a\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_b\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"MatMul_1\"\n",
      "op: \"MatMul\"\n",
      "input: \"y1\"\n",
      "input: \"W2/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_a\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_b\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph_def = g.as_graph_def()\n",
    "for node in graph_def.node:\n",
    "    if node.op == 'MatMul':\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了访问节点定义，我们还可以通过`tf.Grap`的`get_tensor_by_namemethod` 方法来读取每个图中每一节点的内容。这个方法会返回一个`Tensor`对象，可以通过`sess.run`读取其中的内容。请注意：tensor的名字并不完全是对应节点的名字，我们需要附加上[device placement](https://www.tensorflow.org/programmers_guide/graphs#placing_operations_on_different_devices)。 在后面的例子中，权重`W1`被放到`index0`的设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04186983  0.01554188 -0.04810648 ... -0.0057848  -0.04940532\n",
      "   0.04064221]\n",
      " [-0.0357748   0.05161197  0.03711758 ... -0.04559297  0.0094061\n",
      "  -0.00094092]\n",
      " [ 0.04965667  0.02794094  0.04235747 ... -0.0361556   0.00126217\n",
      "  -0.02915845]\n",
      " ...\n",
      " [ 0.01335635  0.00402979 -0.03288927 ... -0.01393164 -0.03195806\n",
      "  -0.01588652]\n",
      " [-0.04886859  0.05068135 -0.01241539 ...  0.04052348  0.05338682\n",
      "  -0.02036126]\n",
      " [-0.00651863 -0.04804414  0.03801457 ... -0.02860732  0.04754426\n",
      "  -0.02520439]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess: # we need to explicitly set the graph or the default graph will be used.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # read the content of initialised W1\n",
    "    W1_tensor = g.get_tensor_by_name('W1:0')\n",
    "    print(sess.run(W1_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要读取取决于`Input`节点的tensor内容，我们需要在会话执行时向其提供内容。可以查阅资料https://www.tensorflow.org/api_docs/python/tf/placeholder, 如果还是不能完全明白TensorFlow，可以看这个网址： [Link](https://github.com/ringochu/TensorFlowDemo)这是一个小的展示，在一张简洁的海报中总结了TensorFlow 以及 NumPy的基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03610415 0.06912688 0.07223938 0.16913089 0.0772726  0.1951995\n",
      "  0.07214345 0.18331014 0.04666999 0.07880298]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # read the final classification result of a random image\n",
    "    logits_tensor = g.get_tensor_by_name('logits:0')\n",
    "    print(sess.run(logits_tensor, feed_dict={input_tensor: np.random.random((28, 28, 1))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN作为计算图\n",
    "\n",
    "基于前面的讨论，我们可以构建卷积神经网络（[CNN](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148)）并在TensorFlow中读取其结构和系数。CNN可以看作是包含卷积层的特殊计算图，在TensorFlow中是`tf.nn.conv2d`。(https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) 可以在本教程中访问**卷积层**原理的详细信息。\n",
    "\n",
    "在传统的CNN网络中，每层的输入和输出都是**特征图**，通常是3-Dtensor(批量输入时为4-D). 一幅特征图可以被看成一个多通道图像，它能够代表自然图像或者从潜在的空间里提取的特征。每个特征图的形状是空间特征图的高度和宽度，也是图像通道的数量。卷积层在每个空间特征映射中执行2D卷积，并将结果汇总在一起用于不同的输出通道。除了卷积层之外，最大池层对于减小特征映射的大小以提取更高级别的特征也同样重要。\n",
    "\n",
    "我们在TensorFlow中构建一个CNN的实例，其用于手写体数字的识别。这个CNN的架构被称为[LeNet](http://yann.lecun.com/exdb/lenet/)。它需要使用MNIST数据集进行训练。类似的教程可以在以下链接找到：(https://www.tensorflow.org/tutorials/deep_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet(images, keep_prob):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: a 4-D tensor that holds batched input images\n",
    "    Return:\n",
    "        A tensor that contains classification probabilities result, and a dictionary\n",
    "        of all intermediate tensors.\n",
    "    \"\"\"    \n",
    "    end_points = {}\n",
    "    # Input shape of 28,28,1 and -1 is just for TF purposes\n",
    "    end_points['images'] = tf.reshape(images, [-1, 28, 28, 1])\n",
    "    \n",
    "    # Define the scope\n",
    "    with tf.variable_scope('conv1'):\n",
    "        # Define the weights for the convolution aka. Kernel size, kernel size, stride and number of filters\n",
    "        w1 = tf.get_variable('weights', [5, 5, 1, 32])\n",
    "        \n",
    "        # Define the bias\n",
    "        b1 = tf.get_variable('biases', [32],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        # Perform the computation and apply ReLU function\n",
    "        # First do the conv2d with weights w1 in the SAME namespace then add the bias, later activation function\n",
    "        end_points['conv1'] = tf.nn.relu(\n",
    "            tf.nn.conv2d(end_points['images'], w1, [1, 1, 1, 1], 'SAME') + b1)\n",
    "    \n",
    "    # Add a max-pooling operation with kernel 2x2 and stride 1\n",
    "    end_points['pool1'] = tf.nn.max_pool(\n",
    "        end_points['conv1'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    with tf.variable_scope('conv2'):\n",
    "        w2 = tf.get_variable('weights', [5, 5, 32, 64])\n",
    "        b2 = tf.get_variable('biases', [64],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['conv2'] = tf.nn.relu(\n",
    "            tf.nn.conv2d(end_points['pool1'], w2, [1, 1, 1, 1], 'SAME') + b2)\n",
    "    end_points['pool2'] = tf.nn.max_pool(\n",
    "        end_points['conv2'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    end_points['flatten'] = tf.reshape(end_points['pool2'], [-1, 7 * 7 * 64])\n",
    "    with tf.variable_scope('fc3'):\n",
    "        w3 = tf.get_variable('weights', [7 * 7 * 64, 1024])\n",
    "        b3 = tf.get_variable('biases', [1024],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['fc3'] = tf.nn.relu(tf.matmul(end_points['flatten'], w3) + b3)\n",
    "        \n",
    "    end_points['dropout'] = tf.nn.dropout(end_points['fc3'], keep_prob)\n",
    "    with tf.variable_scope('fc4'):\n",
    "        w4 = tf.get_variable('weights', [1024, 10])\n",
    "        b4 = tf.get_variable('biases', [10],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['fc4'] = tf.matmul(end_points['fc3'], w4) + b4\n",
    "    \n",
    "    return end_points['fc4'], end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，我们基于MNIST数据集训练了这个CNN网络（参考 https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners)\n",
    "\n",
    "以下是我们用于训练模型的代码段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Loss value of a training batch at step     0: 2.272092\n",
      "Accuracy after running     0 steps: 0.098200\n",
      "Loss value of a training batch at step   100: 2.291337\n",
      "Loss value of a training batch at step   200: 2.254262\n",
      "Loss value of a training batch at step   300: 2.271129\n",
      "Loss value of a training batch at step   400: 2.270882\n",
      "Loss value of a training batch at step   500: 2.252652\n",
      "Loss value of a training batch at step   600: 2.220472\n",
      "Loss value of a training batch at step   700: 2.219046\n",
      "Loss value of a training batch at step   800: 2.204627\n",
      "Loss value of a training batch at step   900: 2.211274\n",
      "Loss value of a training batch at step  1000: 2.205071\n",
      "Accuracy after running  1000 steps: 0.434200\n",
      "Loss value of a training batch at step  1100: 2.217518\n",
      "Loss value of a training batch at step  1200: 2.156273\n",
      "Loss value of a training batch at step  1300: 2.189674\n",
      "Loss value of a training batch at step  1400: 2.161802\n",
      "Loss value of a training batch at step  1500: 2.169288\n",
      "Loss value of a training batch at step  1600: 2.151241\n",
      "Loss value of a training batch at step  1700: 2.142624\n",
      "Loss value of a training batch at step  1800: 2.127059\n",
      "Loss value of a training batch at step  1900: 2.084398\n",
      "Loss value of a training batch at step  2000: 2.090031\n",
      "Accuracy after running  2000 steps: 0.629300\n",
      "Loss value of a training batch at step  2100: 2.112322\n",
      "Loss value of a training batch at step  2200: 2.060897\n",
      "Loss value of a training batch at step  2300: 2.032979\n",
      "Loss value of a training batch at step  2400: 2.064638\n",
      "Loss value of a training batch at step  2500: 2.040615\n",
      "Loss value of a training batch at step  2600: 2.053550\n",
      "Loss value of a training batch at step  2700: 1.976818\n",
      "Loss value of a training batch at step  2800: 1.983986\n",
      "Loss value of a training batch at step  2900: 1.989769\n",
      "Loss value of a training batch at step  3000: 1.967710\n",
      "Accuracy after running  3000 steps: 0.714400\n",
      "Loss value of a training batch at step  3100: 1.901442\n",
      "Loss value of a training batch at step  3200: 1.959940\n",
      "Loss value of a training batch at step  3300: 1.917199\n",
      "Loss value of a training batch at step  3400: 1.954271\n",
      "Loss value of a training batch at step  3500: 1.839952\n",
      "Loss value of a training batch at step  3600: 1.838784\n",
      "Loss value of a training batch at step  3700: 1.857384\n",
      "Loss value of a training batch at step  3800: 1.902746\n",
      "Loss value of a training batch at step  3900: 1.835868\n",
      "Loss value of a training batch at step  4000: 1.852537\n",
      "Accuracy after running  4000 steps: 0.757200\n",
      "Loss value of a training batch at step  4100: 1.783903\n",
      "Loss value of a training batch at step  4200: 1.761977\n",
      "Loss value of a training batch at step  4300: 1.724894\n",
      "Loss value of a training batch at step  4400: 1.736925\n",
      "Loss value of a training batch at step  4500: 1.692347\n",
      "Loss value of a training batch at step  4600: 1.677666\n",
      "Loss value of a training batch at step  4700: 1.632892\n",
      "Loss value of a training batch at step  4800: 1.708302\n",
      "Loss value of a training batch at step  4900: 1.689637\n",
      "Loss value of a training batch at step  5000: 1.748880\n",
      "Accuracy after running  5000 steps: 0.781200\n",
      "Loss value of a training batch at step  5100: 1.571712\n",
      "Loss value of a training batch at step  5200: 1.590313\n",
      "Loss value of a training batch at step  5300: 1.597164\n",
      "Loss value of a training batch at step  5400: 1.573363\n",
      "Loss value of a training batch at step  5500: 1.519158\n",
      "Loss value of a training batch at step  5600: 1.450274\n",
      "Loss value of a training batch at step  5700: 1.464609\n",
      "Loss value of a training batch at step  5800: 1.484366\n",
      "Loss value of a training batch at step  5900: 1.395990\n",
      "Loss value of a training batch at step  6000: 1.422245\n",
      "Accuracy after running  6000 steps: 0.799100\n",
      "Loss value of a training batch at step  6100: 1.378582\n",
      "Loss value of a training batch at step  6200: 1.535506\n",
      "Loss value of a training batch at step  6300: 1.313151\n",
      "Loss value of a training batch at step  6400: 1.342918\n",
      "Loss value of a training batch at step  6500: 1.364226\n",
      "Loss value of a training batch at step  6600: 1.262094\n",
      "Loss value of a training batch at step  6700: 1.289735\n",
      "Loss value of a training batch at step  6800: 1.283792\n",
      "Loss value of a training batch at step  6900: 1.280176\n",
      "Loss value of a training batch at step  7000: 1.262677\n",
      "Accuracy after running  7000 steps: 0.809000\n",
      "Loss value of a training batch at step  7100: 1.180509\n",
      "Loss value of a training batch at step  7200: 1.270477\n",
      "Loss value of a training batch at step  7300: 1.226040\n",
      "Loss value of a training batch at step  7400: 1.177263\n",
      "Loss value of a training batch at step  7500: 1.274649\n",
      "Loss value of a training batch at step  7600: 1.130209\n",
      "Loss value of a training batch at step  7700: 1.143178\n",
      "Loss value of a training batch at step  7800: 1.065004\n",
      "Loss value of a training batch at step  7900: 1.078003\n",
      "Loss value of a training batch at step  8000: 1.026783\n",
      "Accuracy after running  8000 steps: 0.818500\n",
      "Loss value of a training batch at step  8100: 1.053263\n",
      "Loss value of a training batch at step  8200: 1.044198\n",
      "Loss value of a training batch at step  8300: 1.113458\n",
      "Loss value of a training batch at step  8400: 1.024762\n",
      "Loss value of a training batch at step  8500: 1.080957\n",
      "Loss value of a training batch at step  8600: 1.135177\n",
      "Loss value of a training batch at step  8700: 1.024210\n",
      "Loss value of a training batch at step  8800: 0.984887\n",
      "Loss value of a training batch at step  8900: 0.827346\n",
      "Loss value of a training batch at step  9000: 1.045435\n",
      "Accuracy after running  9000 steps: 0.828300\n",
      "Loss value of a training batch at step  9100: 0.962692\n",
      "Loss value of a training batch at step  9200: 1.051836\n",
      "Loss value of a training batch at step  9300: 1.011958\n",
      "Loss value of a training batch at step  9400: 0.851952\n",
      "Loss value of a training batch at step  9500: 0.927596\n",
      "Loss value of a training batch at step  9600: 1.041436\n",
      "Loss value of a training batch at step  9700: 0.823394\n",
      "Loss value of a training batch at step  9800: 0.863912\n",
      "Loss value of a training batch at step  9900: 0.728812\n",
      "Loss value of a training batch at step 10000: 0.943951\n",
      "Accuracy after running 10000 steps: 0.834400\n",
      "Loss value of a training batch at step 10100: 0.902378\n",
      "Loss value of a training batch at step 10200: 0.650948\n",
      "Loss value of a training batch at step 10300: 0.766422\n",
      "Loss value of a training batch at step 10400: 0.690768\n",
      "Loss value of a training batch at step 10500: 0.753523\n",
      "Loss value of a training batch at step 10600: 0.823742\n",
      "Loss value of a training batch at step 10700: 0.759728\n",
      "Loss value of a training batch at step 10800: 0.826319\n",
      "Loss value of a training batch at step 10900: 0.760564\n",
      "Loss value of a training batch at step 11000: 0.810561\n",
      "Accuracy after running 11000 steps: 0.845000\n",
      "Loss value of a training batch at step 11100: 0.700904\n",
      "Loss value of a training batch at step 11200: 0.798044\n",
      "Loss value of a training batch at step 11300: 0.763184\n",
      "Loss value of a training batch at step 11400: 0.569579\n",
      "Loss value of a training batch at step 11500: 0.652691\n",
      "Loss value of a training batch at step 11600: 0.628150\n",
      "Loss value of a training batch at step 11700: 0.621088\n",
      "Loss value of a training batch at step 11800: 0.889441\n",
      "Loss value of a training batch at step 11900: 0.722328\n",
      "Loss value of a training batch at step 12000: 0.785903\n",
      "Accuracy after running 12000 steps: 0.853900\n",
      "Loss value of a training batch at step 12100: 0.739241\n",
      "Loss value of a training batch at step 12200: 0.562231\n",
      "Loss value of a training batch at step 12300: 0.568480\n",
      "Loss value of a training batch at step 12400: 0.767892\n",
      "Loss value of a training batch at step 12500: 0.803342\n",
      "Loss value of a training batch at step 12600: 0.598144\n",
      "Loss value of a training batch at step 12700: 0.748746\n",
      "Loss value of a training batch at step 12800: 0.564249\n",
      "Loss value of a training batch at step 12900: 0.625481\n",
      "Loss value of a training batch at step 13000: 0.654081\n",
      "Accuracy after running 13000 steps: 0.860400\n",
      "Loss value of a training batch at step 13100: 0.644981\n",
      "Loss value of a training batch at step 13200: 0.493810\n",
      "Loss value of a training batch at step 13300: 0.672121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value of a training batch at step 13400: 0.640367\n",
      "Loss value of a training batch at step 13500: 0.711528\n",
      "Loss value of a training batch at step 13600: 0.595643\n",
      "Loss value of a training batch at step 13700: 0.700417\n",
      "Loss value of a training batch at step 13800: 0.572912\n",
      "Loss value of a training batch at step 13900: 0.590981\n",
      "Loss value of a training batch at step 14000: 0.523539\n",
      "Accuracy after running 14000 steps: 0.867300\n",
      "Loss value of a training batch at step 14100: 0.647697\n",
      "Loss value of a training batch at step 14200: 0.564996\n",
      "Loss value of a training batch at step 14300: 0.499261\n",
      "Loss value of a training batch at step 14400: 0.655310\n",
      "Loss value of a training batch at step 14500: 0.479925\n",
      "Loss value of a training batch at step 14600: 0.569616\n",
      "Loss value of a training batch at step 14700: 0.540437\n",
      "Loss value of a training batch at step 14800: 0.602443\n",
      "Loss value of a training batch at step 14900: 0.534764\n",
      "Loss value of a training batch at step 15000: 0.629089\n",
      "Accuracy after running 15000 steps: 0.873500\n",
      "Loss value of a training batch at step 15100: 0.600819\n",
      "Loss value of a training batch at step 15200: 0.590917\n",
      "Loss value of a training batch at step 15300: 0.538974\n",
      "Loss value of a training batch at step 15400: 0.517905\n",
      "Loss value of a training batch at step 15500: 0.515531\n",
      "Loss value of a training batch at step 15600: 0.548911\n",
      "Loss value of a training batch at step 15700: 0.459293\n",
      "Loss value of a training batch at step 15800: 0.642689\n",
      "Loss value of a training batch at step 15900: 0.460582\n",
      "Loss value of a training batch at step 16000: 0.666072\n",
      "Accuracy after running 16000 steps: 0.879300\n",
      "Loss value of a training batch at step 16100: 0.476846\n",
      "Loss value of a training batch at step 16200: 0.493611\n",
      "Loss value of a training batch at step 16300: 0.483149\n",
      "Loss value of a training batch at step 16400: 0.364905\n",
      "Loss value of a training batch at step 16500: 0.472060\n",
      "Loss value of a training batch at step 16600: 0.491306\n",
      "Loss value of a training batch at step 16700: 0.330731\n",
      "Loss value of a training batch at step 16800: 0.528514\n",
      "Loss value of a training batch at step 16900: 0.595236\n",
      "Loss value of a training batch at step 17000: 0.339870\n",
      "Accuracy after running 17000 steps: 0.884600\n",
      "Loss value of a training batch at step 17100: 0.546053\n",
      "Loss value of a training batch at step 17200: 0.443179\n",
      "Loss value of a training batch at step 17300: 0.573740\n",
      "Loss value of a training batch at step 17400: 0.423939\n",
      "Loss value of a training batch at step 17500: 0.423746\n",
      "Loss value of a training batch at step 17600: 0.519343\n",
      "Loss value of a training batch at step 17700: 0.451941\n",
      "Loss value of a training batch at step 17800: 0.363346\n",
      "Loss value of a training batch at step 17900: 0.511457\n",
      "Loss value of a training batch at step 18000: 0.388926\n",
      "Accuracy after running 18000 steps: 0.888000\n",
      "Loss value of a training batch at step 18100: 0.512918\n",
      "Loss value of a training batch at step 18200: 0.538566\n",
      "Loss value of a training batch at step 18300: 0.440529\n",
      "Loss value of a training batch at step 18400: 0.473716\n",
      "Loss value of a training batch at step 18500: 0.519680\n",
      "Loss value of a training batch at step 18600: 0.416953\n",
      "Loss value of a training batch at step 18700: 0.325086\n",
      "Loss value of a training batch at step 18800: 0.468537\n",
      "Loss value of a training batch at step 18900: 0.537575\n",
      "Loss value of a training batch at step 19000: 0.386964\n",
      "Accuracy after running 19000 steps: 0.892400\n",
      "Loss value of a training batch at step 19100: 0.403430\n",
      "Loss value of a training batch at step 19200: 0.400301\n",
      "Loss value of a training batch at step 19300: 0.417411\n",
      "Loss value of a training batch at step 19400: 0.506205\n",
      "Loss value of a training batch at step 19500: 0.672385\n",
      "Loss value of a training batch at step 19600: 0.529585\n",
      "Loss value of a training batch at step 19700: 0.460354\n",
      "Loss value of a training batch at step 19800: 0.357077\n",
      "Loss value of a training batch at step 19900: 0.598003\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You don't need to run this code snippet since we have already trained it\n",
    "# and it will consume lots of resources on our server.\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"../data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    logits, end_points = lenet(images, keep_prob)\n",
    "    \n",
    "    # Nodes for training\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "    train = tf.train.AdadeltaOptimizer(1e-3).minimize(loss)\n",
    "    \n",
    "    # accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(20000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(50)\n",
    "            _, loss_val = sess.run([train, loss],\n",
    "                                   feed_dict={images: batch_xs,\n",
    "                                              labels: batch_ys,\n",
    "                                              keep_prob: 0.5})\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                print('Loss value of a training batch at step %5d: %f' % (i, np.mean(loss_val)))\n",
    "            if i % 1000 == 0:\n",
    "                acc = sess.run(accuracy,\n",
    "                               feed_dict={images: mnist.test.images,\n",
    "                                          labels: mnist.test.labels,\n",
    "                                          keep_prob: 1.0})\n",
    "                print('Accuracy after running %5d steps: %f' % (i, acc))\n",
    "        \n",
    "        # save the trained model\n",
    "        saver.save(sess, \"mnist_lenet_log/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 练习\n",
    "\n",
    "在本练习中，建议你在每个卷积层和最大池化层之后可视化中间特征图。我们提供以下代码片段的基本构架，你需要完成下面的任务：\n",
    "\n",
    "1.\t正确识别lenet函数中所有卷积层和最大池化层的`end_point` tensor。 注意，看到多个划线的地方来寻找网络。 这些tensor可以视为这些层的输出。\n",
    "2.\t根据输入测试图像收集这些tensor的值。 可以从测试数据集（`mnist.test.images`）中选择一个图像\n",
    "3.\t对于每个tensor值，你可以获得tensor任何通道的2D图像。 提示：tensor值是NumPy ndarray。\n",
    "4.\t通过[`plt.imshow`](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html)可视化2D图像。\n",
    "\n",
    "小注：答案在答案文件夹内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_lenet_log/: cannot open `mnist_lenet_log/' (No such file or directory)\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that mnist_lenet_log, the directory contains trained model, exists.\n",
    "!file mnist_lenet_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE VERSION\n",
    "def visualize_tensor(image, key, channel_idx, axis):\n",
    "    \"\"\"\n",
    "    Visualize a tensor in the trained LeNet model.\n",
    "    Args:\n",
    "        image: a test image\n",
    "        key: the key to the tensor in end_points\n",
    "        channel_idx: index of the channel to be visualized\n",
    "        axis: a pyplot Axis object\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with g.as_default():\n",
    "        images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        logits, end_points = lenet(images, keep_prob)\n",
    "\n",
    "        # Nodes for training\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "        train = tf.train.AdadeltaOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "        # accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        with tf.Session(graph=g) as sess:\n",
    "            saver.restore(sess, 'mnist_lenet_log/')\n",
    "            \n",
    "            # TODO: finish the line to get the tensor value of end_points[key]\n",
    "            tensor_val = sess.run(\"\"\"BLANK\"\"\", feed_dict={images: [image], keep_prob: 1.0})\n",
    "            \n",
    "            # TODO: get the 2D image at channel \"channel_idx\"\n",
    "            image_2d = tensor_val[0, \"\"\"BLANK\"\"\"]\n",
    "            \n",
    "            # TODO: visualize\n",
    "            axis.set_title(key)\n",
    "            axis.imshow(\"\"\"BLANK\"\"\", cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFpCAYAAABaoss2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faf9f34ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: use visualize_tensor to visualize the channel 0 of all convolutional layers and max-pooling layers of the first test image in MNIST\n",
    "fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(8, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
